{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global df\n",
    "global alpha\n",
    "global theta\n",
    "global X\n",
    "global y\n",
    "global lmbda\n",
    "global delta\n",
    "global train\n",
    "global test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Function to import data\n",
    "\n",
    "def import_data():\n",
    "    global df\n",
    "    \n",
    "    df = pd.read_excel('C:\\\\Users\\\\asus\\\\Desktop\\\\LR.xlsx')\n",
    "    df.dropna(inplace=True)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Function to standardize input features\n",
    "\n",
    "def stand_norm():\n",
    "    global df\n",
    "    \n",
    "    backup = df\n",
    "    resp = df['MDV']\n",
    "    df.drop(['MDV'], axis=1, inplace=True)\n",
    "    \n",
    "    cols = list(df)\n",
    "    \n",
    "    stats = df.describe()\n",
    "    for col in cols:\n",
    "        def func(x):\n",
    "            return (x-stats[col][1])/stats[col][2]\n",
    "\n",
    "        df[col] = df[col].apply(func)\n",
    "    df['MDV'] = resp\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Function to create 2nd degree polynomial features as L2-Regularization isn't really useful with a few number of features.\n",
    "\n",
    "def feature_map():\n",
    "    global df\n",
    "    \n",
    "    cols = list(df)\n",
    "    \n",
    "    backup = df\n",
    "    resp = df['MDV']\n",
    "    df.drop(['MDV'], axis=1, inplace=True)\n",
    "\n",
    "    nums = range(0,12)\n",
    "    for i in range(0,18):\n",
    "        first = random.choice(nums)\n",
    "        second = random.choice(nums)\n",
    "        name = cols[first] + \"x\" + cols[second]\n",
    "        df[name] = df[cols[first]].multiply(df[cols[second]], axis=0)\n",
    "\n",
    "    df['MDV'] = resp\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to compute J_theta (or the cost function)\n",
    "\n",
    "def costFunc(X,y,theta,length):\n",
    "    global lmbda\n",
    "    \n",
    "    sq_diff = np.sum((np.sum(np.array(X) * theta, axis=1) - np.array(y))**2)\n",
    "    cost = (1/(2*(length))) * (sq_diff + (lmbda * np.sum(theta[1:]**2)))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Function to run batch gradient descent algorithm to optimize the cost fuction of the problem.\n",
    "\n",
    "def gradientDesc(X,y,train,lmbda,alpha,n_iter):\n",
    "    global theta\n",
    "    \n",
    "    costs = list()\n",
    "    for i in range(0, n_iter):\n",
    "        theta[0] = theta[0] - (alpha/len(train)) * np.sum((np.sum(np.array(X) * theta, axis=1) - np.array(y)))\n",
    "        for j in range(1,len(theta)):\n",
    "            theta[j] = theta[j]*(1 - ((alpha*lmbda)/len(train))) - (alpha/len(train)) * (np.sum((np.sum(np.array(X) * theta, axis=1) - np.array(y)) * np.array(X.ix[:,j])))\n",
    "        costs.append(costFunc(X, y, theta, len(train)))\n",
    "    return costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main(A=None):\n",
    "    global alpha\n",
    "    global df\n",
    "    global theta\n",
    "    global X\n",
    "    global y\n",
    "    global lmbda\n",
    "    global delta\n",
    "    global train\n",
    "    global test\n",
    "    \n",
    "    lmbda = 1\n",
    "    alpha = 0.005\n",
    "    n_iter = 1500\n",
    "    \n",
    "    #To import, standardize and map features to achieve final data set.\n",
    "    import_data()\n",
    "    stand_norm()\n",
    "    feature_map()\n",
    "    \n",
    "    \n",
    "    df = df.iloc[np.random.permutation(len(df)-1)].reset_index(drop=1) # To shuffle the dataset\n",
    "    train_size = int(round(len(df) * 0.75)) # Training set size: 75% of full data set.\n",
    "    train = df[:train_size]\n",
    "    test = df[train_size:]\n",
    "    \n",
    "    Xn    = pd.Series([1] * len(train)) # Creation feature X_0 which is to hold a value of 1.\n",
    "    X     = pd.concat([Xn, train.drop(['MDV'], axis=1)], axis=1) #Feature set in X\n",
    "    y     = train['MDV'] #Response variable in y\n",
    "    theta = [0.0] * (len(list(X))) #Theta or parameter vector initialized with 1\n",
    "    theta = np.array(theta)\n",
    "    \n",
    "    #To train on train data set\n",
    "    costs = gradientDesc(X,y,train,lmbda,alpha,n_iter)\n",
    "    \n",
    "    Xn    = pd.Series([1] * len(test))\n",
    "    test.reset_index(drop=1, inplace=True)\n",
    "    test_x = pd.concat([Xn, test.drop('MDV',axis=1)], axis=1)\n",
    "    test_y = test['MDV']\n",
    "    \n",
    "    #To test model on test set\n",
    "    predictions = test_x * theta\n",
    "    residuals = np.sum(np.array(test_x) * theta, axis=1) - test_y\n",
    "    residuals = [abs(x) for x in residuals]\n",
    "    cost = sum(residuals)/len(residuals)\n",
    "    \n",
    "    return cost\n",
    "    #Normal Equation method comparison\n",
    "    #theta_num  = np.dot(np.dot((np.linalg.inv(np.dot(np.matrix.transpose(np.array(X)),np.array(X)))), np.matrix.transpose(np.array(X))), np.array(y))\n",
    "    #cost = costFunc(X,y,theta_num)\n",
    "    \n",
    "    #print 'Mean Absolute Percentage Error of model on Test set:',abs(cost),'%'\n",
    "    #plt.plot(costs)\n",
    "    #plt.title('J_Theta(For Training) vs Number of iterations')\n",
    "    #plt.ylabel('J_Theta(Cost)')\n",
    "    #plt.xlabel('Number of Iterations')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of Mean Absolute Percentage Error of model after 10 iterations: 3.58469752812 %\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    costs = list()\n",
    "    \n",
    "    for i in range(0,10):\n",
    "        \n",
    "        costs.append(main())\n",
    "        \n",
    "    print 'Average of Mean Absolute Percentage Error of model after 10 iterations:',sum(costs)/len(costs),'%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now we use the built-in ridge regression module in sci-kit learn on our data set to compare performance with the one we built above.\n",
    "\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Percentage Error of built-in model on test set: 3.35436974736 %\n"
     ]
    }
   ],
   "source": [
    "clf = Ridge(alpha=1, solver='sag', max_iter=1500) #We use the stochastic averaged gradient descent algorihtm since batch GD isn't available\n",
    "model = clf.fit(X, y)\n",
    "\n",
    "Xn    = pd.Series([1] * len(test))\n",
    "test.reset_index(drop=1, inplace=True)\n",
    "test_x = pd.concat([Xn, test.drop('MDV',axis=1)], axis=1)\n",
    "test_y = test['MDV']\n",
    "\n",
    "pred = model.predict(test_x)\n",
    "residuals = pred - test_y\n",
    "residuals = [abs(x) for x in residuals]\n",
    "cost = sum(residuals)/len(residuals)\n",
    "print 'Mean Absolute Percentage Error of built-in model on test set:',cost,'%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the performance of both the models are fairly similar, and , and therefore conclude that our implementation is correct, if not better than the built-in module."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
